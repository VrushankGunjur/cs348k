### Project Proposal: Fast Web Diffusion
Vrushank Gunjur (vrushank), Alexander Waitz (waitz), Nahum Maru (nmaru972) for CS348k


## Summary: Make web diffusion fast.
We would like to produce an in-browser “viewfinder” for stable diffusion. As the user enters a prompt, our application will produce low-resolution images generated by client-side stable diffusion for quick visual feedback (analogous to how a camera viewfinder provides an instant approximation of the final image). If the user stops modifying the prompt for a sufficient amount of time, the application will generate a higher-resolution image to replace the low-resolution stand-in image. We will achieve fast front-end performance by compiling our diffusion code as optimized, WebGPU-accelerated WASM bytecode. 

## Inputs and Outputs:
	The fundamental constraints of our project are twofold. First, to get a diffusion model to run fast enough to allow for a new paradigm of interacting with text-to-image diffusion models: using a rapid-rendering viewfinder to make the prompting process more productive. This is a constraint on machine performance, the inference time for low-resolution images must be on the order of a few seconds on a modern consumer-grade laptop, fast enough for iteration. The second constraint is to get this model to run in-browser, so that inference uses client-side compute resources and doesn’t offload the expensive inference computation to a server the model provider is responsible for. This constraint limits the size of the models we can use to the limit on WebAssembly modules (now 4GB), and requires us to make use of WebGPU to get access to client-side hardware. We believe that this constraint is important to make the diffusion model as low-cost and accessible as possible. As a side constraint, we lack the compute resources and time to train these large models ourselves, so we rely on post-training techniques and systems-level optimizations to improve inference performance. Thus, our inputs are text prompts entered to a website, and the output is a series of images generated in-browser, each of increasing complexity until a final image is presented. As the text prompt changes, ideally the output image will also update in “real time”.

## Inputs and outputs: 
List the inputs and outputs of the implementation, as well as the major constraints on your design (is the fundamental constraint machine performance? is it human time?). In this section I want you to take lessons from the papers we've read that did a good job of articulating this problem setup.






## Task List:
Develop a simple website that takes in a prompt as input and as an output, returns a corresponding image. The Stable Diffusion LCM-LoRA model will be used to generate the images and will be run via the Hugging Face API. 
Become acquainted with Web-GPU and define the maximum size that the model must be to fit into a WebAssembly page.
Select a small-enough version of Stable Diffusion LCM-LoRA to fit these constraints.
Using Web-GPU, accelerate Stable Diffusion LCM-LoRA to run locally and benchmark how long it takes for one step of LCM-LoRA to run and the quality of it (i.e. does it provide enough information to the user to be a worthwhile experience). 
Explore techniques like model quantization or pruning that could help reduce the model size without significantly compromising on performance.
Benchmark latency for running LCM-LoRA for multiple steps to achieve the final image.
Develop and implement the view-finding logic to synchronize the running of singular steps of LCM LoRA (for viewfinding) with the multi-step final creation of the image to maintain a smooth user flow.
Implement the front-end UI changes to deliver the view-finding experience. 
Explore and implement further optimizations to reduce latency and improve the quality of the experience.

## Nice-to-haves:
Capabilities to handle more advanced prompts — such as negative prompts
“Real-time” displaying of view-finder images
Assignments:
Nahum: Looks post-training optimizations and distillation methods  
Vrushank: TVM/GPU optimization s+ Web Development
Alex: TVM/GPU optimizations + Web Development
Expected Deliverables:
	Our primary deliverable will be the stable diffusion viewfinder interface. The user will be able to enter a prompt and rapidly be shown a generated image. As they type, the displayed image will update in tandem. If the user ceases to type, a higher-resolution generated image will populate within 30 seconds. We would like this application to run smoothly on a modern Apple silicon MacBook Pro or any other system with similar CPU/GPU resources.
Alongside this interactive demo we will produce quantitative benchmarks on the performance and runtime of our system. This will include statistics on correlation between the stand-in and final images and comparative runtimes between the low- and high-resolution image generation times.

## Biggest Risks:
	Since we have a baseline for web-based image diffusion, we know that a badly-performing version of this is possible. Thus, the biggest risks are failures in getting this solution to work quickly. We are relying on recent research in the field, such as LCM-LoRA and other distillation/acceleration techniques (which we hope to learn from in the process) to accelerate the model to work with existing open-source models that don’t require training, and for these methods to be accessible enough to implement in reasonable time in an efficient WebGPU implementation. Since we’re also limited by module size, another risk is that these techniques don’t require the use of a model too large to fit in the browser. If we can’t get the diffusion model to run fast enough to have some semblance of real-time performance, we can’t realize our goal of a functional “viewfinder” for generating images. If this were the case, we might have to set further limitations on the hardware this solution runs on or the quality of the outputs.
What are the biggest risks? Please document the biggest risks in the project. Often there are points or blockers such as, if I can't get this code to compile/run, then I can't do the work. Or, until I am successfully training this DNN and have a reasonably trained model, I can't do anything else. I want you to think through the risks on your project, and consider how to eliminate/derisk these aspects of the project with as little work as possible.

## What you need help with?
	We primarily need help on finding techniques that don’t require training our own model, that would accelerate inference. We’re primarily interested in how we can modify/distill open-source models and their inference steps to push their speed. If we have to train a small model, we might need access to a GPU – this would also be helpful in benchmarking the performance of our implementation on a GPU. All of us only have access to Apple MacBooks, and can thus only test on low-grade Apple hardware. Since we’re fundamentally developing an application that should work on heterogeneous hardware, it’d be nice to have access to a few different environments to test in.
